# -*- coding: utf-8 -*-
"""NoticiasEnerg√≠aOEC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DEnCLAsc3XDnkw3X9dMyH0uEipBEsq2a
"""

# Instalaci√≥n de librer√≠as necesarias
!pip install requests beautifulsoup4 --quiet

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from datetime import datetime
from IPython.display import display
from google.colab import files

# Headers para evitar bloqueos
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \
                   (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "es-ES,es;q=0.9",
    "Connection": "keep-alive",
}

# Palabras clave positivas y negativas
keywords = [ "energ√≠a", "transici√≥n energ√©tica", "transici√≥n sostenible", "renovables", "nextgeneration",
    "sostenibilidad", "electricidad", "calor", "potencia", "nuclear", "solar", "e√≥lica",
    "hidroel√©ctrica", "eficiencia energ√©tica", "aerogenerador", "aerotermia", "fotovoltaico",
    "biomasa", "energ√≠a t√©rmica", "energ√≠a el√©ctrica", "geotermia", "almacenamiento hidroel√©ctrico",
    "almacenamiento t√©rmico", "emisiones", "carbono", "descarbonizaci√≥n", "gases", "invernadero",
    "instalaciones", "energ√≠a verde", "fuentes de energ√≠a", "autoconsumo", "placas solares",
    "veh√≠culos el√©ctricos", "veh√≠culos h√≠bridos" ]

exclusion_keywords = [ "subvenci√≥n", "ayuda", "guerra", "militar", "ej√©rcito", "misil", "bomba", "ataque",
    "conflicto", "israel", "palestina", "iran", "ucrania", "rusia", "otan", "norte corea", "nuclear militar",
    "armamento", "defensa", "netanyahu", "trump", "putin", "ham√°s", "hezbol√°", "suministro", "impacto", "fuentes" ]

# Regex
keywords_regex = re.compile(r'\b(?:' + '|'.join(re.escape(kw) for kw in keywords) + r')\b', re.IGNORECASE)
exclusion_regex = re.compile(r'\b(?:' + '|'.join(re.escape(kw) for kw in exclusion_keywords) + r')\b', re.IGNORECASE)

def is_relevant(title):
    return bool(keywords_regex.search(title)) and not exclusion_regex.search(title)

# Plantilla para scraping con headers y fecha de extracci√≥n
def run_scraper(nombre_medio, url, selector, base_url=None, attrs=None):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        articles = soup.select(selector) if not attrs else soup.find_all(attrs["tag"], class_=attrs["class"])
        results = []
        for a in articles:
            if not a or not a.get_text(strip=True): continue
            title = a.get_text(strip=True)
            link = a['href'] if a.has_attr('href') else None
            if not link:
                continue
            if base_url and not link.startswith("http"):
                link = base_url + link
            if is_relevant(title):
                results.append({
                    "medio": nombre_medio,
                    "t√≠tulo": title,
                    "url": link,
                    "fecha_extraccion": datetime.now().strftime("%Y-%m-%d")
                })
        print(f"[‚úî] {nombre_medio}: {len(results)} noticia(s) relevante(s) encontrada(s).")
        return results
    except Exception as e:
        print(f"[‚ö†Ô∏è] Error en {nombre_medio}: {e}")
        return []

# Lista de medios
medios = [
    ("EnergyNews", "https://www.energynews.es/", "article h2.entry-title a", None),
    ("EFEVerde", "https://efeverde.com/", "article h2 a", None),
    ("El Peri√≥dico de la Energ√≠a", "https://elperiodicodelaenergia.com/", "h3.entry-title a", None),
    ("Energ√≠as Renovables", "https://www.energias-renovables.com/", "div.enrTitularNoticia a", "https://www.energias-renovables.com"),
    ("Review Energy", "https://www.review-energy.com/", "div.card-title a", None),
    ("Diario de la Energ√≠a", "https://www.diariodelaenergia.com/", "h3.entry-title a", None),
    ("El D√≠a", "https://www.eldia.es/", "article h3 a", "https://www.eldia.es"),
    ("Diario de Avisos", "https://diariodeavisos.elespanol.com/", "h2.entry-title a", None),
    ("El Pa√≠s", "https://elpais.com/", "h2.c_t a", "https://elpais.com"),
    ("El Mundo", "https://www.elmundo.es/", "h2.ue-c-cover-content__headline a", "https://www.elmundo.es"),
    ("La Vanguardia", "https://www.lavanguardia.com/", "h2 a", "https://www.lavanguardia.com"),
    ("ABC", "https://www.abc.es/", "h2 a", "https://www.abc.es"),
    ("EFE", "https://efe.com/", "article h3 a", "https://efe.com"),
    ("Europa Press", "https://www.europapress.es/", "div.noticiacuerpo h2 a", "https://www.europapress.es"),
    ("La Raz√≥n", "https://www.larazon.es/", "h2.title-news a", "https://www.larazon.es"),
    ("El Espa√±ol", "https://www.elespanol.com/", "article h2 a", "https://www.elespanol.com"),
    ("Cadena SER Canarias", "https://cadenaser.com/ccaa/canarias/", "article h2 a", "https://cadenaser.com"),
    ("Euronews UE", "https://es.euronews.com/tag/union-europea", "main a", "https://es.euronews.com"),
    ("EnergiaParaElCambio", "https://energiaparaelcambio.com/", "main a", "https://energiaparaelcambio.com"),
]

# Ejecutar scraping
news = []
for nombre, url, selector, base_url in medios:
    news += run_scraper(nombre, url, selector, base_url)

# Mostrar resultados
df = pd.DataFrame(news)

if df.empty:
    print("\nüö´ No se encontraron noticias recientes con las palabras clave.")
else:
    print(f"\n‚úÖ Total de noticias relevantes encontradas: {len(df)}\n")
    pd.set_option('display.max_colwidth', None)
    display(df)

    # Guardar CSV y Excel
    hoy = datetime.now().strftime("%Y-%m-%d")
    csv_file = f"noticias_energia_{hoy}.csv"
    excel_file = f"noticias_energia_{hoy}.xlsx"

    df.to_csv(csv_file, index=False)
    df.to_excel(excel_file, index=False)

    files.download(csv_file)
    files.download(excel_file)